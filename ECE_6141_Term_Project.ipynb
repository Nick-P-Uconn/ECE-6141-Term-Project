{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPAuyA/f4QnzAJzxTfj0fQ0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nick-P-Uconn/ECE-6141-Term-Project/blob/main/ECE_6141_Term_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0H8aUh8_rx8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "320ee80b-124b-4aa0-d5ee-6f0ae298b4d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/6.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.2/6.9 MB\u001b[0m \u001b[31m127.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/76.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hâœ… Libraries installed successfully\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# ðŸ“¦ ENVIRONMENT SETUP\n",
        "# ============================================================\n",
        "!pip install open_spiel torch matplotlib numpy pandas --quiet\n",
        "\n",
        "import torch, pyspiel, numpy as np, matplotlib.pyplot as plt, pandas as pd\n",
        "print(\"âœ… Libraries installed successfully\")\n",
        "\n",
        "# Optional: Mount Google Drive to save outputs\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import Open Spiel Backgammon and torch\n",
        "import pyspiel, torch, torch.nn as nn, torch.optim as optim\n",
        "import numpy as np, random, pandas as pd\n",
        "from collections import deque\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# configuration varibles\n",
        "n_episodes = 200000\n",
        "discount_rate = 0.99 #gamma\\\n",
        "#Inteval to test against random opponent to ensure improving win rate\n",
        "eval_interval = 2000\n",
        "# Numebr of test Games per evaluation\n",
        "rand_test_Num_Games = 100\n",
        "\n",
        "#Rand_opening is used to ensure that over traning various early/mid game states are encountered and are also rewarded\n",
        "#this ensure the model has some training on all levels of the game\n",
        "#caps the amount of moves madde to ensure that the game is not in end game to start\n",
        "rand_opening_move_cap = 8\n",
        "\n",
        "# Exploration schedules\n",
        "#epsilon and Tau are set high early to implore the model to make mistakes while learning\n",
        "#as time goes on Epsilon deccrease resulting in minmal changes to the model\n",
        "max_epsilon, min_epsilon, epsilon_decay_rate = 1, 0.05, 75000   # epsilon\n",
        "\n",
        "#as time goes on decrease Tau to ensure the model only picks the most win likely\n",
        "#scenario in accordance with its training\n",
        "#reduce to 1e-3  or less to produce an effective argmax for training\n",
        "max_tau, min_tau = 8, 0.1                    # softmax temperature\n",
        "\n",
        "\n",
        "\n",
        "#import the game of backgammon fromm Open SPiel\n",
        "game = pyspiel.load_game(\"backgammon\")\n",
        "\n",
        "#code obtained from open spiel documentation\n",
        "def reset_to_playable_state():\n",
        "    #sets the game to the intial board state\n",
        "    s = game.new_initial_state()\n",
        "    while s.is_chance_node():\n",
        "        # chance_outcomes() -> list[(action_id, prob)]\n",
        "        a = random.choice(s.chance_outcomes())[0]\n",
        "        s.apply_action(a)\n",
        "    return s\n",
        "\n",
        "\n",
        "def new_state(rand_opening=True):\n",
        "   #calls the intial board state\n",
        "    s = reset_to_playable_state()\n",
        "    #if in traning perform random moves\n",
        "    if rand_opening:\n",
        "        for _ in range(random.randint(0, rand_opening_move_cap)):\n",
        "            #break if game has ended somehow\n",
        "            if s.is_terminal(): break\n",
        "            #roll dice\n",
        "            if s.is_chance_node():\n",
        "                a = random.choice(s.chance_outcomes())[0]\n",
        "                s.apply_action(a)\n",
        "                continue\n",
        "            #checks if there is a legal move based on the dice roll\n",
        "            acts = s.legal_actions()\n",
        "            #if a legal move exists choose one of the random actions\n",
        "            if acts:\n",
        "                s.apply_action(random.choice(acts))\n",
        "            else:\n",
        "                break\n",
        "    return s\n",
        "\n",
        "# Player-0 observation length\n",
        "input_dim = len(reset_to_playable_state().observation_tensor(0))\n",
        "\n",
        "# ---------------------------- Value Network ----------------------------------\n",
        "class ValueNet(nn.Module):\n",
        "    def __init__(self, dim, h=256):\n",
        "        super().__init__()\n",
        "        #creates a nerual network with 2 hidden layers one with 256 neurons the other with\n",
        "        #128 neurons activation function for each is a rectified linear unit\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, h), nn.ReLU(),\n",
        "            nn.Linear(h, h//2), nn.ReLU(),\n",
        "            #final layer acts as the state output essentially what move should we make\n",
        "            nn.Linear(h//2, 1)\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "value_net = ValueNet(input_dim)\n",
        "#use an adam optimizer with a learning rate of .001 for the nerual network\n",
        "optimizer = optim.Adam(value_net.parameters(), lr=1e-4)\n",
        "#Loss=(1/N)âˆ‘â€‹(VÎ¸â€‹(siâ€‹)âˆ’targetiâ€‹)^2\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "\n",
        "def get_eps_tau(ep):\n",
        "    #determines the epsilon and tau values for a given episode\n",
        "    #based on the decay rate and episode\n",
        "    #Note max function ensure neither epsilon or taur ever get to low in a given episode\n",
        "    epsilon = max(min_epsilon, max_epsilon - (max_epsilon - min_epsilon)*(ep/ epsilon_decay_rate))\n",
        "    tau = max(min_tau, max_tau - (max_tau - min_tau)*(ep/epsilon_decay_rate))\n",
        "    return epsilon, tau\n",
        "\n",
        "def select_action(state, ep, softmax=True):\n",
        "    epsilon, tau = get_eps_tau(ep)\n",
        "\n",
        "    # If we're at a chance node, resolve it first (shouldn't happen in training loop,\n",
        "    # but safe-guard in case of randomized openings landing on a chance node)\n",
        "    while state.is_chance_node():\n",
        "        a = random.choice(state.chance_outcomes())[0]\n",
        "        state.apply_action(a)\n",
        "\n",
        "    legal = state.legal_actions()\n",
        "    if not legal:\n",
        "        return None, epsilon, tau\n",
        "\n",
        "    if random.random() < epsilon:\n",
        "        return random.choice(legal), epsilon, tau\n",
        "\n",
        "    vals = []\n",
        "    for a in legal:\n",
        "        child = state.child(a)\n",
        "        obs = torch.tensor(child.observation_tensor(0), dtype=torch.float32)\n",
        "        with torch.no_grad():\n",
        "            vals.append(value_net(obs).item())\n",
        "    if softmax:\n",
        "        probs = torch.softmax(torch.tensor(vals)/tau, dim=0)\n",
        "        idx = torch.multinomial(probs, 1).item()\n",
        "    else:\n",
        "        idx = int(np.argmax(vals))\n",
        "    return legal[idx], epsilon, tau\n",
        "\n",
        "# ---------------------------- Episode Rollout --------------------------------\n",
        "def play_episode(ep, gamma=discount_rate):\n",
        "    # start new game\n",
        "    s = new_state(rand_opening=True)\n",
        "\n",
        "    total_r = 0.0\n",
        "    n_steps = 0\n",
        "    epsilon = tau = None\n",
        "\n",
        "    # while game is going\n",
        "    while not s.is_terminal():\n",
        "        # select the move (epsilon / tau schedule)\n",
        "        a, epsilon, tau = select_action(s, ep, softmax=True)\n",
        "\n",
        "        # Handle no-legal-actions (pass) edge case\n",
        "        if a is None:\n",
        "            acts = s.legal_actions()\n",
        "            if acts:\n",
        "                s.apply_action(acts[0])\n",
        "            continue\n",
        "\n",
        "        # current state observation\n",
        "        obs = torch.tensor(s.observation_tensor(0), dtype=torch.float32)\n",
        "\n",
        "        # apply chosen action\n",
        "        s.apply_action(a)\n",
        "\n",
        "        # reward from this step\n",
        "        r = s.rewards()[0]\n",
        "        total_r += r\n",
        "\n",
        "        # next state observation\n",
        "        nxt_obs = torch.tensor(s.observation_tensor(0), dtype=torch.float32)\n",
        "        done = s.is_terminal()\n",
        "\n",
        "        # ------------- TD(0) UPDATE (online) -------------\n",
        "        with torch.no_grad():\n",
        "            target_val = r if done else r + gamma * value_net(nxt_obs).item()\n",
        "\n",
        "        target = torch.tensor([target_val], dtype=torch.float32)\n",
        "\n",
        "        pred = value_net(obs)\n",
        "        loss = criterion(pred, target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "    return total_r, epsilon, tau, loss.item()\n",
        "\n",
        "\n",
        "# ---------------------------- Evaluation -------------------------------------\n",
        "def random_action(state):\n",
        "    #aqpplys a random action when evaluating against the random player\n",
        "    while state.is_chance_node():\n",
        "        a = random.choice(state.chance_outcomes())[0]\n",
        "        state.apply_action(a)\n",
        "    acts = state.legal_actions()\n",
        "    return random.choice(acts) if acts else None\n",
        "\n",
        "def evaluate_vs_random(agent_net, n_games=50):\n",
        "    wins = 0\n",
        "    for _ in range(n_games):\n",
        "        s = new_state(rand_opening=False)\n",
        "        while not s.is_terminal():\n",
        "            while s.is_chance_node():\n",
        "                a = random.choice(s.chance_outcomes())[0]\n",
        "                s.apply_action(a)\n",
        "            if s.current_player() == 0:\n",
        "                legal = s.legal_actions()\n",
        "                if not legal:\n",
        "                    # pass if environment says so\n",
        "                    s.apply_action(s.legal_actions()[0]) if s.legal_actions() else None\n",
        "                else:\n",
        "                    vals = []\n",
        "                    for a in legal:\n",
        "                      #as this is the player choose the actions with best probaility according to RL model\n",
        "                        child = s.child(a)\n",
        "                        obs = torch.tensor(child.observation_tensor(0), dtype=torch.float32)\n",
        "                        #note model returns best moves in a read only mode do not want to allow updates\n",
        "                        #based on performance against purley randomized opponent\n",
        "                        with torch.no_grad():\n",
        "                            vals.append(agent_net(obs).item())\n",
        "                    a = legal[int(np.argmax(vals))]\n",
        "                    s.apply_action(a)\n",
        "            else:\n",
        "              #random player thus use randomized action\n",
        "                a = random_action(s)\n",
        "                if a is not None:\n",
        "                    s.apply_action(a)\n",
        "        #if the model won add one to the win column\n",
        "        if s.rewards()[0] > 0:\n",
        "            wins += 1\n",
        "    return wins / n_games\n",
        "\n",
        "\n",
        "#note this is effectivley equivelant to polaying rnadom just using a save state\n",
        "#to continue to show agents improvment through training\n",
        "def evaluate_net_vs_net(net_A, net_B, n_games=50):\n",
        "    wins_A = 0\n",
        "    for g in range(n_games):\n",
        "        s = new_state(rand_opening=False)\n",
        "\n",
        "        # alternate player order for fairness\n",
        "        if g % 2 == 0:\n",
        "            p0, p1 = net_A, net_B\n",
        "        else:\n",
        "            p0, p1 = net_B, net_A\n",
        "\n",
        "        while not s.is_terminal():\n",
        "            while s.is_chance_node():\n",
        "                a = random.choice(s.chance_outcomes())[0]\n",
        "                s.apply_action(a)\n",
        "\n",
        "            legal = s.legal_actions()\n",
        "            if not legal:\n",
        "                break\n",
        "\n",
        "            # pick network for the player to move\n",
        "            net = p0 if s.current_player() == 0 else p1\n",
        "\n",
        "            vals = []\n",
        "            for a in legal:\n",
        "                child = s.child(a)\n",
        "                obs = torch.tensor(child.observation_tensor(0), dtype=torch.float32)\n",
        "                with torch.no_grad():\n",
        "                    vals.append(net(obs).item())\n",
        "            a = legal[int(np.argmax(vals))]\n",
        "            s.apply_action(a)\n",
        "\n",
        "        r0 = s.rewards()[0]\n",
        "        #determine if the network under training one if so then we are doing a good job technically\n",
        "        if g % 2 == 0:\n",
        "            if r0 > 0: wins_A += 1\n",
        "        else:\n",
        "            if r0 < 0: wins_A += 1\n",
        "\n",
        "    return wins_A / n_games\n",
        "\n",
        "\n",
        "losses, rewards, epsilons, taus, winrates = [], [], [], [], []\n",
        "rw = deque(maxlen=50)\n",
        "\n",
        "for ep in range(1, n_episodes + 1):\n",
        "\n",
        "    # play one episode and update online during it\n",
        "    final_r, epsilon, tau, avg_loss = play_episode(ep, discount_rate)\n",
        "\n",
        "    # log epsilon/tau once per episode\n",
        "    if epsilon is None or tau is None:\n",
        "        epsilon, tau = get_eps_tau(ep)\n",
        "\n",
        "    epsilons.append(epsilon)\n",
        "    taus.append(tau)\n",
        "\n",
        "    # one TD_Loss per episode: average loss over that episode's moves\n",
        "    losses.append(avg_loss)\n",
        "    rewards.append(final_r)\n",
        "    rw.append(final_r)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # if a certain amount of games has been played then call the evaluation\n",
        "    if (ep % eval_interval) == 0 or ep == 10 or (ep < 500 and ep % (eval_interval/4) == 0):\n",
        "\n",
        "        # at ep = 50000, save the baseline model once\n",
        "        if ep == 50000:\n",
        "            Path(\"checkpoints\").mkdir(exist_ok=True)\n",
        "            torch.save(value_net.state_dict(), \"checkpoints/value_net_ep50000.pth\")\n",
        "\n",
        "        # choose evaluation opponent\n",
        "        if ep < 50000:\n",
        "\n",
        "            wr = evaluate_vs_random(value_net, n_games=rand_test_Num_Games)\n",
        "            opp_label = \"Random\"\n",
        "        else:\n",
        "            # at/after 50000: evaluate\n",
        "            baseline = ValueNet(input_dim)\n",
        "            baseline.load_state_dict(torch.load(\"checkpoints/value_net_ep50000.pth\"))\n",
        "            wr = evaluate_net_vs_net(value_net, baseline, n_games=rand_test_Num_Games)\n",
        "            opp_label = \"EP50000 Model\"\n",
        "\n",
        "        winrates.append((ep, wr))\n",
        "        # prints the parameters used in this set of win-rate evaluation\n",
        "        print(f\"Ep {ep:4d}/{n_episodes} | Îµ={epsilons[-1]:.3f} Ï„={taus[-1]:.2f} | \"\n",
        "              f\"Loss={losses[-1]:.4f} | AvgR(50)={np.mean(rw):+.3f} | \"\n",
        "              f\"WinRate vs {opp_label} = {wr:.2f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"Training complete\")\n",
        "\n",
        "# ---------------------------- Save Model ------------------------------------\n",
        "out = Path(\"checkpoints\")\n",
        "out.mkdir(exist_ok=True)\n",
        "\n",
        "model_path = out/\"value_net.pth\"\n",
        "\n",
        "torch.save({\n",
        "    \"model_state\": value_net.state_dict(),\n",
        "    \"opt_state\": optimizer.state_dict()\n",
        "}, model_path)\n",
        "\n",
        "# save CSV\n",
        "df = pd.DataFrame({\n",
        "    \"Episode\": np.arange(1, n_episodes + 1),\n",
        "    \"TD_Loss\": losses,\n",
        "    \"Reward\": rewards,\n",
        "    \"Epsilon\": epsilons,\n",
        "    \"Tau\": taus,\n",
        "    \"SmoothedReward\": pd.Series(rewards).rolling(50).mean()\n",
        "})\n",
        "\n",
        "for ep_wr, wr in winrates:\n",
        "    df.loc[df[\"Episode\"] == ep_wr, \"WinRate\"] = wr\n",
        "\n",
        "df.to_csv(out/\"training_log.csv\", index=False)\n",
        "\n",
        "# ---------------------------- Download Model ---------------------------------\n",
        "from google.colab import files\n",
        "files.download(str(model_path))\n",
        "files.download(\"checkpoints/training_log.csv\")\n",
        "\n"
      ],
      "metadata": {
        "id": "C4j2WVij_x_y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "e9c3f910-20b6-4cb9-8fc3-6b2f82808166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Ep   10/200000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3200196667.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}